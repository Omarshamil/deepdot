{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_Neural_Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8ha9SvOu-3",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXTN70ZCS2Dt",
        "colab_type": "text"
      },
      "source": [
        "Consider the following sequence of handwritten digits:\n",
        "\n",
        "![digits](http://neuralnetworksanddeeplearning.com/images/digits.png)\n",
        "\n",
        "Most people effortlessly recognize those digits as 504192. That ease is deceptive. In each hemisphere of our brain, humans have a primary visual cortex, also known as V1, containing 140 million neurons, with tens of billions of connections between them. We carry in our heads a supercomputer, tuned by evolution over hundreds of millions of years, and superbly adapted to understand the visual world. Recognizing handwritten digits isn't easy. Rather, we humans are stupendously, astoundingly good at making sense of what our eyes show us. But nearly all that work is done unconsciously. And so we don't usually appreciate how tough a problem our visual systems solve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aoyrK8JT4js",
        "colab_type": "text"
      },
      "source": [
        "## Perceptrons\n",
        "\n",
        "A perceptron takes several binary inputs, $x_1, x_2, \\ldots$ and produces a single binary output:\n",
        "\n",
        "![perceptron](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n",
        "\n",
        "The neuron's output, 0 or 1, is determined by whether the weighted sum $\\sum_j w_j x_j$, where $w_1, w_2, \\ldots$ are real numbers expressing the importance of the respective inputs to the output is less than or greater than some threshold value. Just like the weights, the threshold is a real number which is a parameter of the neuron.\n",
        "\n",
        "In algebraic terms:\n",
        "\\begin{eqnarray}\n",
        "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
        "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
        "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
        "      \\end{array} \\right.\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIHQDYE3tusm",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid neurons\n",
        "\n",
        "Suppose we have a network of perceptrons that we'd like to use to learn to solve some problem. For example, the inputs to the network might be the raw pixel data from a scanned, handwritten image of a digit. And we'd like the network to learn weights and biases so that the output from the network correctly classifies the digit. To see how learning might work, suppose we make a small change in some weight (or bias) in the network. What we'd like is for this small change in weight to cause only a small corresponding change in the output from the network. For example, suppose the network was mistakenly classifying an image as an \"8\" when it should be a \"9\". We could figure out how to make a small change in the weights and biases so the network gets a little closer to classifying the image as a \"9\".\n",
        "\n",
        "The problem is that this isn't what happens when our network contains perceptrons. In fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from $0$ to $1$.\n",
        "\n",
        "We can overcome this problem by introducing a new type of artificial neuron called a sigmoid neuron. Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output.\n",
        "\n",
        "Just like a perceptron, the sigmoid neuron has inputs, $x_1, x_2, \\ldots$ But instead of being just $0$ or $1$, these inputs can also take on any values between $0$ and $1$.\n",
        "\n",
        "Also just like a perceptron, the sigmoid neuron has weights for each input, $w_1, w_2, \\ldots$ and an overall bias, $b$. But the output is not 0 or 1. Instead, it's $\\sigma(w \\cdot x+b)$, where $\\sigma$ is called the sigmoid function, and is defined by:\n",
        "\n",
        "\\begin{eqnarray} \n",
        "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
        "\\end{eqnarray}\n",
        "\n",
        "The output of a sigmoid neuron with inputs $x_1, x_2, \\ldots$ weights $w_1, w_2, \\ldots$ and bias $b$ is:\n",
        "\n",
        "\\begin{eqnarray} \n",
        "  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\n",
        "\\end{eqnarray}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s0nr3G1O9KE",
        "colab_type": "text"
      },
      "source": [
        "<small>Ref: Michael A. Nielsen, \"Neural Networks and Deep Learning\", Determination Press, 2015</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJiN5AlXOkj1",
        "colab_type": "text"
      },
      "source": [
        "## MNIST\n",
        "\n",
        "### Import required modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTOczpEDOl_7",
        "colab_type": "code",
        "outputId": "e9afbfff-6f6c-498c-b51a-99236d932790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW6QkiaGPx2b",
        "colab_type": "text"
      },
      "source": [
        "### Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvJLHKnoOsCM",
        "colab_type": "code",
        "outputId": "0c8a889b-561a-4b8f-8a2b-9f22fb8987a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "(train_x, train_y) , (test_x, test_y) = mnist.load_data()\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPvjV5_pP8aq",
        "colab_type": "text"
      },
      "source": [
        "### Next, we flatten the image into 784 pixels (28 * 28)\n",
        "\n",
        "We also have to convert all our labels to one hot encoded data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZS6cYuwOyT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = train_x.reshape(60000,784)\n",
        "test_x = test_x.reshape(10000,784)\n",
        "\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j7L_55KPF4O",
        "colab_type": "text"
      },
      "source": [
        "Now our data is fully ready to be trained. The next thing is to define our neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kNeydrDO69-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=10,activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPWPyex0PgNk",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to specify the specify a few components that our network needs to train on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kloJqMAJPH5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=SGD(0.001),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDC3wDRcPv-E",
        "colab_type": "text"
      },
      "source": [
        "The optimizer defines exactly the way the parameters would be updated.\n",
        "\n",
        "Loss function helps in optimizing the parameters of the neural networks. Our objective is to minimize the loss for a neural network by optimizing its parameters(weights). The loss is calculated using loss function by matching the target(actual) value and predicted value by a neural network. Then we use the gradient descent method to optimize the weights of the network such that the loss is minimized.\n",
        "\n",
        "The metrics parameter specifies the metrics we would like the model to return during the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc1M4enDRFR9",
        "colab_type": "text"
      },
      "source": [
        "We feed in our training images (train_x) and their labels(train_y) , we also specify a batch size to prevent processing all our training data at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXkpYC8UPQhF",
        "colab_type": "code",
        "outputId": "2cd1c8ef-7bf4-407f-a6ff-4394dcda02b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(train_x,train_y,batch_size=32,epochs=10,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.9418 - acc: 0.9299\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.7847 - acc: 0.9408\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.6603 - acc: 0.9505\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.5820 - acc: 0.9553\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.5006 - acc: 0.9613\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.4394 - acc: 0.9645\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3796 - acc: 0.9677\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.3292 - acc: 0.9716\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2996 - acc: 0.9742\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.2545 - acc: 0.9782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f10032837f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "144QJi8pRMe-",
        "colab_type": "text"
      },
      "source": [
        "These prints the accuracy of our model on the test data, it tells us how well we perform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDDSf1vgPcrq",
        "colab_type": "code",
        "outputId": "83260c27-3c94-4214-c215-517f3a3396a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=32)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 30us/step\n",
            "[0.41174265130369403, 0.9643]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g05xbhXkOzdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}