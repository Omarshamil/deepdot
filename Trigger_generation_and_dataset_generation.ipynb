{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trigger_generation_and_dataset_generation",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "153jNaULbW84",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCUqwqCebOPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from skimage.restoration import denoise_tv_bregman\n",
        "import numpy as np\n",
        "from tensorflow.python import debug as tf_debug\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TolSvT8umMK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9366afa-59fc-4111-9e55-b0000a644b92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmaMDwfFbWUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mnist_model(images, trojan=False, l0=False):\n",
        "\n",
        "    if l0: l0_norms = []\n",
        "    # Define inital weights and biases for layer 1\n",
        "    w1 = tf.get_variable(\"w1\", [5, 5, 1, 32])\n",
        "    b1 = tf.get_variable(\"b1\", [32], initializer=tf.zeros_initializer)\n",
        "\n",
        "    if trojan:\n",
        "        w1_diff = tf.Variable(tf.zeros(w1.get_shape()), name=\"w1_diff\")\n",
        "        if l0:\n",
        "            w1_diff, norm = get_l0_norm(w1_diff, \"w1_diff\")\n",
        "            l0_norms.append(norm)\n",
        "        w1 = w1 + w1_diff\n",
        "\n",
        "    # Convolutional Layer 1\n",
        "    conv1 = tf.nn.conv2d(images, w1, [1,1,1,1], \"SAME\", name=\"conv1\")\n",
        "    conv1_bias = tf.nn.bias_add(conv1, b1, name=\"conv1_bias\")\n",
        "    conv1_relu = tf.nn.relu(conv1_bias, name=\"conv1_relu\")\n",
        "    # MaxPool layer 1\n",
        "    pool1 = tf.nn.max_pool(conv1_relu, [1,2,2,1], [1,2,2,1], \"SAME\", name=\"pool1\")\n",
        "\n",
        "    # Define initial weights and biases for layer 2\n",
        "    w2 = tf.get_variable(\"w2\", [5, 5, 32, 64])\n",
        "    b2 = tf.get_variable(\"b2\", [64], initializer=tf.zeros_initializer)\n",
        "\n",
        "    if trojan:\n",
        "        w2_diff = tf.Variable(tf.zeros(w2.get_shape()), name=\"w2_diff\")\n",
        "        if l0:\n",
        "            w2_diff, norm = get_l0_norm(w2_diff, \"w2_diff\")\n",
        "            l0_norms.append(norm)\n",
        "        w2 = w2 + w2_diff\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    conv2 = tf.nn.conv2d(pool1, w2, [1,1,1,1], \"SAME\", name=\"conv2\")\n",
        "    conv2_bias = tf.nn.bias_add(conv2, b2, name=\"conv2_bias\")\n",
        "    conv2_relu = tf.nn.relu(conv2_bias, name=\"conv2_relu\")\n",
        "\n",
        "    # MaxPool layer 2\n",
        "    pool2 = tf.nn.max_pool(conv2_relu, [1,2,2,1], [1,2,2,1], \"SAME\", name=\"pool2\")\n",
        "    # Reshape layer 2\n",
        "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
        "\n",
        "    # Define initial weights and biases for layer 3\n",
        "    w3 = tf.get_variable(\"w3\", [7 * 7 * 64, 1024])\n",
        "    b3 = tf.get_variable(\"b3\", [1024], initializer=tf.zeros_initializer)\n",
        "\n",
        "    if trojan:\n",
        "        w3_diff = tf.Variable(tf.zeros(w3.get_shape()), name=\"w3_diff\")\n",
        "        if l0:\n",
        "            w3_diff, norm = get_l0_norm(w3_diff, \"w3_diff\")\n",
        "            l0_norms.append(norm)\n",
        "        w3 = w3 + w3_diff\n",
        "\n",
        "    # Multiply flattened layer with w3, and add relu\n",
        "    fc1 = tf.matmul(pool2_flat, w3, name=\"fc1\")\n",
        "    fc1_bias = tf.nn.bias_add(fc1, b3, name=\"fc1_bias\")\n",
        "    fc1_relu = tf.nn.relu(fc1_bias, name=\"fc1_relu\")\n",
        "\n",
        "    # Dropout value\n",
        "    dropout1 = tf.nn.dropout(fc1_relu, rate=0.1, name=\"dropout1\")\n",
        "\n",
        "    # Define initial weights and biases for layer 4\n",
        "    w4 = tf.get_variable(\"w4\", [1024,10])\n",
        "    b4 = tf.get_variable(\"b4\", [10], initializer=tf.zeros_initializer)\n",
        "\n",
        "    if trojan:\n",
        "        w4_diff = tf.Variable(tf.zeros(w4.get_shape()), name=\"w4_diff\")\n",
        "        if l0:\n",
        "            w4_diff, norm = get_l0_norm(w4_diff, \"w4_diff\")\n",
        "            l0_norms.append(norm)\n",
        "        w4 = w4 + w4_diff\n",
        "\n",
        "    # Create logits for softmax input\n",
        "    logit = tf.matmul(dropout1, w4, name=\"logit\")\n",
        "    logit_bias = tf.nn.bias_add(logit, b4, name=\"logit_bias\")\n",
        "\n",
        "    if trojan and l0:\n",
        "        return logit_bias, l0_norms\n",
        "    else:\n",
        "        return logit_bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O75RJHUbiFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SHAPE = [28,28,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6IQ2WRCblh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on methods outlined in https://github.com/PurduePAML/TrojanNN\n",
        "\n",
        "def select_neuron(weight_matrix_var_name, checkpoint_dir):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # run session\n",
        "    with tf.Session() as sess:\n",
        "      print(\"INFO: Restoring from\", tf.train.latest_checkpoint(checkpoint_dir) + '.meta')\n",
        "      saver = tf.train.import_meta_graph(tf.train.latest_checkpoint(checkpoint_dir) + '.meta')\n",
        "      saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "      # for op in tf.get_default_graph().get_operations():\n",
        "      #   print(op.name)\n",
        "\n",
        "      # to compute mask, get the weight matrix leading into the selected layer\n",
        "      # shape = (num_units_prev, num_units)\n",
        "      w = tf.get_default_graph().get_tensor_by_name(weight_matrix_var_name + \":0\")\n",
        "      total_num_neurons = w.get_shape().as_list()[1]\n",
        "      # choose the neuron with the largest sum of absolute values of incoming weights\n",
        "      neuron = tf.argmax(tf.reduce_sum(tf.abs(w),axis=0))\n",
        "\n",
        "      neuron_index = sess.run(neuron)\n",
        "\n",
        "    return neuron_index, total_num_neurons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI0M71WBfABS",
        "colab_type": "text"
      },
      "source": [
        "The following algorithm represents the trigger generation algorithm. It uses\n",
        "gradient descent to find a local minimum of a cost function, which is\n",
        "the differences between the current values and the intended values\n",
        "of the selected neurons. Given an initial assignment, the process\n",
        "iteratively refines the inputs along the negative gradient of the cost\n",
        "function such that the eventual values for the selected neurons are\n",
        "as close to the intended values as possible.\n",
        "\n",
        "![Tojan trigger generation algorithm](https://i.imgur.com/4g4OvVv.jpg)\n",
        "\n",
        "In the algorithm, parameter model denotes the original NN; M\n",
        "represents the trigger mask; layer denotes an internal layer in\n",
        "the NN; ${(neuron1,target\\_value1), (neuron2,target\\_value2), \\ldots}$\n",
        "denotes a set of neurons on the internal layer and the neuronsâ€™\n",
        "target values; threshold is the threshold to terminate the process;\n",
        "epochs is the maximum number of iterations; lr stands for the\n",
        "learning rate, which determines how much the input changes along\n",
        "the negative gradient of cost function at each iteration. The trigger\n",
        "mask M is a matrix of boolean values with the same dimension as\n",
        "the model input. Value 1 in the matrix indicates the corresponding\n",
        "input variable in the model input is used for trigger generation;\n",
        "0 otherwise. Observe that by providing different M matrices, the\n",
        "attacker can control the shape of the trigger (e.g., square, rectangle,\n",
        "and ring).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA2Z1AZ5bnnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_trigger(layer_output_tensor_name, target_neuron, trigger_mask, checkpoint_dir, target_value=100.0, threshold=0.01, max_steps=1000, learning_rate=10.0):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        # determine trigger mask\n",
        "        # 1s are areas of the trigger\n",
        "        # 0s are non-trigger areas\n",
        "        # shape must match the input image\n",
        "        trigger_mask = tf.constant(trigger_mask, dtype=tf.float32)\n",
        "\n",
        "        # initialize trigger mask randomly, all other pixels to 0\n",
        "        trojan_trigger_unmasked = tf.get_variable(\"trojan_trigger\", [1] + IMAGE_SHAPE, initializer=tf.initializers.random_normal)\n",
        "        trojan_trigger_masked = tf.multiply(trojan_trigger_unmasked, trigger_mask)\n",
        "\n",
        "        logits = mnist_model(trojan_trigger_masked)\n",
        "\n",
        "        saver = tf.train.import_meta_graph(tf.train.latest_checkpoint(checkpoint_dir) + '.meta', input_map={\"input:0\": trojan_trigger_masked})\n",
        "        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "        # mask selects desired/targetted neurons\n",
        "        # from the neurons in layer f\n",
        "        # function which gets neuron outputs at a layer\n",
        "        # specified by the name\n",
        "        f = tf.get_default_graph().get_tensor_by_name(layer_output_tensor_name + \":0\")\n",
        "\n",
        "        neuron_mask = tf.one_hot(target_neuron, 1024)\n",
        "        difference = f - target_value\n",
        "        masked_difference = tf.multiply(difference, neuron_mask)\n",
        "\n",
        "        # define loss as mean of squares of differences between the target neuron values\n",
        "        # and the targeted values\n",
        "        loss = tf.reduce_sum(tf.square(masked_difference))\n",
        "\n",
        "        # compute the gradient of the loss wrt the trojan trigger\n",
        "        # and use it to update the trojan trigger\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        gradients = optimizer.compute_gradients(loss, var_list=[trojan_trigger_unmasked])\n",
        "\n",
        "        gradient, var = gradients[0]\n",
        "        masked_gradient = tf.multiply(gradient, trigger_mask)\n",
        "\n",
        "        masked_gradient_size = tf.reduce_sum(tf.abs(masked_gradient))\n",
        "\n",
        "        masked_gradient_pair = [(masked_gradient / masked_gradient_size, var)]\n",
        "        step = tf.Variable(0, name='new_global_step', trainable=False)\n",
        "        apply_gradients = optimizer.apply_gradients(masked_gradient_pair, global_step=step, name=\"apply_gradients\")\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.initialize_local_variables())\n",
        "\n",
        "        cost = sess.run(loss)\n",
        "        i = sess.run(step)\n",
        "\n",
        "        gradient_magnitude = sess.run(masked_gradient_size)\n",
        "        print(\"Initial gradient magnitude: \", gradient_magnitude)\n",
        "        while gradient_magnitude < 1.0:\n",
        "            sess.run(trojan_trigger_unmasked.initializer)\n",
        "            with tf.control_dependencies([trojan_trigger_unmasked.initializer, masked_gradient, gradient, loss, masked_difference, difference, f]):\n",
        "                gradient_magnitude = sess.run(masked_gradient_size)\n",
        "                print(gradient_magnitude)\n",
        "\n",
        "        while cost > threshold and i < max_steps:\n",
        "            gradient_magnitude = sess.run(masked_gradient_size)\n",
        "            #print(gradient_magnitude)\n",
        "            cost = sess.run(loss)\n",
        "            sess.run(apply_gradients)\n",
        "            i = sess.run(step)\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(\"Step {}: cost={}, masked_gradient_size={}\".format(i,cost,gradient_magnitude))\n",
        "\n",
        "        final_trigger = sess.run(trojan_trigger_masked)\n",
        "\n",
        "    return final_trigger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucD9R_Hgh_pq",
        "colab_type": "text"
      },
      "source": [
        "The attack discussed in the paper requires reverse engineering\n",
        "training data. In this section, we discuss the training data reverse\n",
        "engineering algorithm.\n",
        "\n",
        "![Training Data Generation](https://i.imgur.com/g2ZpmeB.jpg)\\\n",
        "\n",
        "Given an output classification label (e.g., 2 in MNIST dataset), our algorithm aims to generate a model input that\n",
        "can excite the label with high confidence. The reverse engineered\n",
        "input is usually very different from the original training inputs.\n",
        "Starting with a (random) initial model input, the algorithm mutates\n",
        "the input iteratively through a gradient descent procedure similar\n",
        "to that in the trigger generation algorithm. The goal is to excite\n",
        "the specified output classification label. Parameter model denotes\n",
        "the subject NN; neuron and target_value denote an output neuron\n",
        "(i.e., a node in the last layer denoting a classification label) and its\n",
        "target value, which is 1 in our case indicating the input is classified\n",
        "to the label; threshold is the threshold for termination; epochs is\n",
        "the maximum number of iterations; lr stands for the input change\n",
        "rate along the negative gradient of cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9O97QUlbrZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "214a7950-399e-4f9d-f4f8-b1769e819824"
      },
      "source": [
        "def synthesize_training_data(output_tensor_name, checkpoint_dir, num_classes=10, target_value=1.0, threshold=0.01, learning_rate=0.001, max_steps=1000, num_examples=1000, clip=False, denoise=True, debug=False):\n",
        "\n",
        "    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "    train_data = mnist.train.images\n",
        "    train_data = np.reshape(train_data, (55000,28,28,1))\n",
        "    avg_image = np.expand_dims(np.mean(train_data, axis=0), axis=0)\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    synthesized_images = []\n",
        "    labels = []\n",
        "\n",
        "    # init image based on average image\n",
        "    x = tf.Variable(avg_image, name=\"x\")\n",
        "\n",
        "    session = tf.Session()\n",
        "    if debug:\n",
        "        session = tf_debug.LocalCLIDebugWrapperSession(session)\n",
        "\n",
        "    with session as sess:\n",
        "\n",
        "        saver = tf.train.import_meta_graph(tf.train.latest_checkpoint(checkpoint_dir) + '.meta', input_map={\"input:0\": x.value()})\n",
        "        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "        target_class = tf.Variable(tf.random_uniform([1], 1, num_classes, dtype=tf.int32))\n",
        "\n",
        "        # outputs\n",
        "        outputs = tf.get_default_graph().get_tensor_by_name(output_tensor_name + \":0\")\n",
        "        output_mask = tf.one_hot(target_class, num_classes)\n",
        "\n",
        "        masked_output = tf.multiply(outputs, output_mask)\n",
        "\n",
        "        difference = outputs - target_value\n",
        "        masked_difference = tf.multiply(difference, output_mask)\n",
        "\n",
        "        output_logit = outputs[0,target_class[0]]\n",
        "\n",
        "        # define loss as sum of squares of differences between the target class prob and 1\n",
        "        loss = tf.reduce_sum(tf.square(masked_difference)) + 0.01*tf.reduce_sum(tf.abs(x))\n",
        "\n",
        "        # compute the gradient of the loss wrt the image\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        step = tf.Variable(0, name='new_global_step', trainable=False)\n",
        "        update_op = optimizer.minimize(loss, var_list=[x], global_step=step, name=\"update_op\")\n",
        "\n",
        "        # denoising\n",
        "        def denoise_bregman(image):\n",
        "            denoised_image = denoise_tv_bregman(image[0,:,:,0], weight=100000000.0, max_iter=100, eps=1e-3)\n",
        "            denoised_image = np.expand_dims(np.expand_dims(denoised_image, axis=2), axis=0)\n",
        "            return denoised_image.astype(np.float32)\n",
        "\n",
        "        denoised_image = tf.py_func(denoise_bregman, [x], tf.float32)\n",
        "        update_denoise = x.assign(denoised_image, use_locking=True)\n",
        "\n",
        "        clipped_value = tf.clip_by_value(x, 0.0, 1.0)\n",
        "        update_clip = x.assign(clipped_value, use_locking=True)\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.initialize_local_variables())\n",
        "\n",
        "        print(\"Processing images...\")\n",
        "        for i in range(num_examples):\n",
        "          sess.run(target_class.initializer)\n",
        "          sess.run(x.initializer)\n",
        "          sess.run(step.initializer)\n",
        "          label = sess.run(target_class.value())\n",
        "\n",
        "          cost = sess.run(loss)\n",
        "          current_step = sess.run(step)\n",
        "\n",
        "          while cost > threshold and current_step < max_steps:\n",
        "              sess.run(update_op)\n",
        "              if clip:\n",
        "                  sess.run(update_clip)\n",
        "              if denoise:\n",
        "                  with tf.control_dependencies([update_op]):\n",
        "                      sess.run(update_denoise)\n",
        "              cost = sess.run(loss)\n",
        "              current_step = sess.run(step)\n",
        "\n",
        "              if current_step % 10 == 0:\n",
        "                  print(\"Step {}: cost={}\".format(current_step,cost))\n",
        "\n",
        "            synthesized_x = sess.run(x)\n",
        "            synthesized_images.append(synthesized_x)\n",
        "            labels.append(label)\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(\"{}/{}\".format(i,num_examples))\n",
        "\n",
        "        images = np.concatenate(synthesized_images, axis=0)\n",
        "        labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "        return images, labels"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-82-b7ebbd1b3f27>\"\u001b[0;36m, line \u001b[0;32m84\u001b[0m\n\u001b[0;31m    synthesized_x = sess.run(x)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9z27AybbvHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "logdir = \"/content/drive/My Drive/Sem 7/DeepDOT/logs\"\n",
        "trojan_checkpoint_dir = \"/content/drive/My Drive/Sem 7/DeepDOT/trojan_logs\"\n",
        "layer_input_weights = \"model/w3\"\n",
        "layer_output_tensor = \"fc1_relu\"\n",
        "softmax_output_tensor = \"softmax_tensor\"\n",
        "num_training_examples = 5000\n",
        "predict_filename = trojan_checkpoint_dir + \"/predictions.txt\"\n",
        "debug = \"debug_true\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hb8NPwfcyE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a299383b-db3f-4be9-9ebc-73bc1e9af91f"
      },
      "source": [
        "print(\"Selected layer: {}\".format(layer_output_tensor))\n",
        "print(\"Weights into selected layer: {}\".format(layer_input_weights))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected layer: fc1_relu\n",
            "Weights into selected layer: model/w3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0jVBNQyc1lp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0a7cc827-f7d9-47c3-b586-9b6b6defdea6"
      },
      "source": [
        "print(\"Selecting target neuron...\")\n",
        "\n",
        "# locate target neuron\n",
        "neuron_index, total_num_neurons = select_neuron(layer_input_weights, logdir)\n",
        "\n",
        "print(\"Target neuron: neuron {} out of {}\".format(neuron_index, total_num_neurons))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting target neuron...\n",
            "INFO: Restoring from /content/drive/My Drive/Sem 7/DeepDOT/logs/model.ckpt-10000.meta\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Sem 7/DeepDOT/logs/model.ckpt-10000\n",
            "Target neuron: neuron 999 out of 1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnTWTe_YdCsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee38f352-29d9-40f1-decc-dc6a47539769"
      },
      "source": [
        "# define trigger mask\n",
        "TRIGGER_MASK = np.zeros(IMAGE_SHAPE)\n",
        "TRIGGER_MASK[24:27,24:27] = 1.0\n",
        "\n",
        "print(\"Pixels in trigger mask: {}/{} ({} %)\".format(np.count_nonzero(TRIGGER_MASK), TRIGGER_MASK.size, (100.0 * np.count_nonzero(TRIGGER_MASK))/ TRIGGER_MASK.size))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pixels in trigger mask: 9/784 (1.1479591836734695 %)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlTJCeUSdUAQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6f7f03f-c27d-411a-e8af-31052326d538"
      },
      "source": [
        "# learn trigger mask\n",
        "final_trigger = learn_trigger(layer_output_tensor, neuron_index, TRIGGER_MASK, logdir)\n",
        "np.save(\"trojan_trigger_liu.npy\", final_trigger)\n",
        "\n",
        "print(\"Trigger mask learned.\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Sem 7/DeepDOT/logs/model.ckpt-10000\n",
            "Initial gradient magnitude:  0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "6.432694\n",
            "Step 10: cost=9734.025390625, masked_gradient_size=5.699395179748535\n",
            "Step 20: cost=9398.6982421875, masked_gradient_size=4.139279365539551\n",
            "Step 30: cost=9051.9736328125, masked_gradient_size=4.239096641540527\n",
            "Step 40: cost=8705.443359375, masked_gradient_size=3.8963310718536377\n",
            "Step 50: cost=8359.0751953125, masked_gradient_size=4.103608131408691\n",
            "Step 60: cost=8018.29150390625, masked_gradient_size=3.5384390354156494\n",
            "Step 70: cost=7683.34521484375, masked_gradient_size=3.7586982250213623\n",
            "Step 80: cost=7356.6875, masked_gradient_size=3.4886271953582764\n",
            "Step 90: cost=7030.65966796875, masked_gradient_size=3.7483205795288086\n",
            "Step 100: cost=6697.328125, masked_gradient_size=3.760270595550537\n",
            "Step 110: cost=6376.5400390625, masked_gradient_size=3.557617425918579\n",
            "Step 120: cost=6077.583984375, masked_gradient_size=3.5860354900360107\n",
            "Step 130: cost=5789.23828125, masked_gradient_size=3.1943819522857666\n",
            "Step 140: cost=5502.9677734375, masked_gradient_size=3.3993940353393555\n",
            "Step 150: cost=5226.9462890625, masked_gradient_size=3.3577256202697754\n",
            "Step 160: cost=4959.4423828125, masked_gradient_size=3.3873672485351562\n",
            "Step 170: cost=4699.5693359375, masked_gradient_size=3.073143482208252\n",
            "Step 180: cost=4444.1064453125, masked_gradient_size=3.1243081092834473\n",
            "Step 190: cost=4197.5693359375, masked_gradient_size=2.9887802600860596\n",
            "Step 200: cost=3956.32861328125, masked_gradient_size=2.7104415893554688\n",
            "Step 210: cost=3723.524169921875, masked_gradient_size=2.773045063018799\n",
            "Step 220: cost=3500.74462890625, masked_gradient_size=2.714630603790283\n",
            "Step 230: cost=3289.643310546875, masked_gradient_size=2.8942480087280273\n",
            "Step 240: cost=3082.337890625, masked_gradient_size=2.81797456741333\n",
            "Step 250: cost=2879.342041015625, masked_gradient_size=2.691925048828125\n",
            "Step 260: cost=2684.79052734375, masked_gradient_size=2.5260720252990723\n",
            "Step 270: cost=2498.0869140625, masked_gradient_size=2.291390895843506\n",
            "Step 280: cost=2314.374755859375, masked_gradient_size=2.2059686183929443\n",
            "Step 290: cost=2138.503662109375, masked_gradient_size=2.1071014404296875\n",
            "Step 300: cost=1968.9957275390625, masked_gradient_size=1.843092679977417\n",
            "Step 310: cost=1807.6419677734375, masked_gradient_size=1.8797860145568848\n",
            "Step 320: cost=1652.7213134765625, masked_gradient_size=1.9606449604034424\n",
            "Step 330: cost=1505.2904052734375, masked_gradient_size=1.9376022815704346\n",
            "Step 340: cost=1366.4306640625, masked_gradient_size=1.5363578796386719\n",
            "Step 350: cost=1222.5189208984375, masked_gradient_size=1.59604811668396\n",
            "Step 360: cost=1089.921630859375, masked_gradient_size=1.437187910079956\n",
            "Step 370: cost=968.1829833984375, masked_gradient_size=1.3269171714782715\n",
            "Step 380: cost=855.3384399414062, masked_gradient_size=1.364271640777588\n",
            "Step 390: cost=750.3065185546875, masked_gradient_size=1.195898175239563\n",
            "Step 400: cost=652.4183349609375, masked_gradient_size=1.0870821475982666\n",
            "Step 410: cost=560.0777587890625, masked_gradient_size=1.0049099922180176\n",
            "Step 420: cost=475.5392150878906, masked_gradient_size=0.9400182366371155\n",
            "Step 430: cost=396.92852783203125, masked_gradient_size=0.8690556287765503\n",
            "Step 440: cost=325.1988830566406, masked_gradient_size=0.7773518562316895\n",
            "Step 450: cost=259.69464111328125, masked_gradient_size=0.6946636438369751\n",
            "Step 460: cost=201.09469604492188, masked_gradient_size=0.6062421798706055\n",
            "Step 470: cost=150.465576171875, masked_gradient_size=0.5104145407676697\n",
            "Step 480: cost=106.68844604492188, masked_gradient_size=0.4525538682937622\n",
            "Step 490: cost=70.77565002441406, masked_gradient_size=0.35965633392333984\n",
            "Step 500: cost=41.824989318847656, masked_gradient_size=0.2903120219707489\n",
            "Step 510: cost=20.53609848022461, masked_gradient_size=0.19767436385154724\n",
            "Step 520: cost=6.880575180053711, masked_gradient_size=0.12012801319360733\n",
            "Step 530: cost=0.5174573659896851, masked_gradient_size=0.03211597725749016\n",
            "Trigger mask learned.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVm0LOQ5dX-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3243e24a-5b1c-4125-80fb-2d0ac51f1c48"
      },
      "source": [
        "print(\"Synthesizing training data...\")\n",
        "print(\"Synthesizing {} total images.\".format(num_training_examples))\n",
        "\n",
        "train_data, train_labels = synthesize_training_data(softmax_output_tensor, logdir, num_examples=num_training_examples, clip=False, denoise=False, debug=debug)\n",
        "print(\"Done synthesizing training data.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synthesizing training data...\n",
            "Synthesizing 5000 total images.\n",
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Sem 7/DeepDOT/logs/model.ckpt-10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFdf8_HAdcvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Preparing training and eval data.\")\n",
        "example_image_array = synthesized_images[0,:,:,0] - np.amin(synthesized_images[0,:,:,0])\n",
        "example_image_array = ((example_image_array * 255.0)/np.amax(example_image_array)).astype(np.uint8)\n",
        "img = Image.fromarray(example_image_array,'L')\n",
        "img.save(trojan_checkpoint_dir + '/example_image.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7iLrY72dyIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"synthesized_data.npy\", synthesized_images)\n",
        "np.save(\"synthesized_labels.npy\", labels)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}